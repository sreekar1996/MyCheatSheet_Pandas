{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pandas and NumPy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "#Importing Libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import cut_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1 = smf.ols(formula= 'quality ~ fixed_acidity + volatile_acidity + alcohol' , data = wine_data_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking outliers at 25%,50%,75%,90%,95% and 99%\n",
    "num_telecom.describe(percentiles=[.25,.5,.75,.90,.95,.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier treatment for Recency\n",
    "plt.boxplot(RFM.Recency)\n",
    "Q1 = RFM.Recency.quantile(0.25)\n",
    "Q3 = RFM.Recency.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "RFM = RFM[(RFM.Recency >= Q1 - 1.5*IQR) & (RFM.Recency <= Q3 + 1.5*IQR)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing value treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data.smokingstatus.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.isnull().sum()*100/retail.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'na': np.nan},inplace=True, regex=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the percentage of missing values\n",
    "round(100*(telecom.isnull().sum()/len(telecom.index)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'na': np.nan}, inplace=True, regex=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.gender, data.smokingstatus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort\n",
    " d=data.sort_values('entery').tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getname(name):\n",
    "    title_group = {'Malee':'Male',\"Male\":\"Male\",\"Female\":\"Female\"}  \n",
    "    return title_group[name]\n",
    "data['gender']=data.gender.map(lambda x : getname(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing value for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('gender')['smokingstatus'].value_counts() \n",
    "data['smokingstatus'].fillna(data['smokingstatus'].value_counts().index[0],inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing treatment based on conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_salery_median=data.groupby(['gender','agegroup']).sbpn.transform('median')\n",
    "age_salery_median.head()\n",
    "data.salery.fillna(age_sbpn_median, inplace=True)\n",
    "#or\n",
    "train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#df = df.drop('class', axis=1)\n",
    "\n",
    "imputer = Imputer(missing_values=np.nan, strategy='median', axis=0)\n",
    "for column in df:\n",
    "    df[[column]] = imputer.fit_transform(df[[column]])\n",
    "    \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupBy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('month')['duration'].sum() # produces Pandas Series\n",
    "data.groupby('month')[['duration']].sum() # Produces Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modi.groupby('year_month')[['sandeshsoldi']].sum().plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monetary Function\n",
    "monetary = order_wise.groupby(\"CustomerID\").Amount.sum()\n",
    "monetary = monetary.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.add_suffix('_Count').reset_index()   #to convert into df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# changing col names and subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking subset from df\n",
    "columns = ['FlyAsh', 'Superplasticize', 'Age', 'FineAggregate', 'CoarseAggregate']\n",
    "testConcreteX = pd.DataFrame(testConcrete1, columns=columns) \n",
    "testConcreteX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming\n",
    "data = pd.read_csv(\"Red Wine.csv\")\n",
    "data.head()\n",
    "data.columns=['fixed_acidity','total_sulfur_dioxide','density','ph','sulphates','alcohol','quality']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_matrix=sparse_matrix.toarray()\n",
    "df_dtm=pd.DataFrame(np_matrix,columns=cv.get_feature_names())\n",
    "df_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(labels = \"patient_id\", axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple features\n",
    "telecom = telecom.drop(['Contract','PaymentMethod','gender','MultipleLines', 'StreamingMovies'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data7=data7.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom = telecom[~np.isnan(telecom['TotalCharges'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.smokingstatus=pd.get_dummies(dff.smokingstatus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dummy variable for the variable 'Contract' and dropping the first one.\n",
    "cont = pd.get_dummies(telecom['Contract'],prefix='Contract',drop_first=True)\n",
    "#Adding the results to the master dataframe\n",
    "telecom = pd.concat([telecom,cont],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(df, cols):\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Yes to 1 and No to 0\n",
    "telecom['PhoneService'] = telecom['PhoneService'].map({'Yes': 1, 'No': 0})\n",
    "telecom['PaperlessBilling'] = telecom['PaperlessBilling'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n",
    "                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n",
    "                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\n",
    "for dataset in train_test_data:\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maping when categories are objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in data.columns: # Loop through all columns in the dataframe\n",
    "    if data[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
    "        data[feature] = pd.Categorical(data[feature]).codes # Replace strings with an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_df['diagnosis'] = bc_df.diagnosis.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(dff.age).head()\n",
    "wholedata1[\"Temp high (°C)\"]=pd.to_numeric(wholedata1.iloc[:,3], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['TotalCharges'] =telecom['TotalCharges'].convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(range(22)):\n",
    "    wholedata.iloc[:,i]=pd.to_numeric(wholedata.iloc[:,i], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO date\n",
    "# parse date\n",
    "#We are using infer_datetime_format=True to read parse the date data, this is slower than using a pre-defined format.\n",
    "retail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dff[[\"age\",\"gender\",\"dbpn\",\"sbpn\",\"agegroup\",\"smokingstatus\",\"Diseasecode\",\"durationofline1\",\"line1drugs\"]]\n",
    "label = dff[\"curestatus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging on 'customerID'\n",
    "df_1 = pd.merge(churn_data, customer_data, how='inner', on='customerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##merging\n",
    "dat=pd.merge(data1,data2)\n",
    "data=pd.merge(dat,fdata)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging amount in order_wise\n",
    "order_wise = pd.concat(objs = [order_wise, amount], axis = 1, ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.merge(df,items,on='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df1=drop_df.append(drop_df)\n",
    "drop_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melting and Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melting\n",
    "df2 = pd.melt(mapping.drop(labels=\"Key\",axis=1), id_vars=[\"Business Area\",\"Category\",\"Legacy company\"], \n",
    "                  var_name=\"SBU\", value_name=\"factor\") # var_name = all the remaining columns to row on SBU column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final table \n",
    "\n",
    "Business Area\tCategory\tLegacy company\tSBU\tfactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot\n",
    "=inf.pivot_table(index=['Source','Plant', 'Plant Name', 'Source Region','Destination'], columns='SBU').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar\n",
    "data3.Gender.value_counts().plot(kind=\"bar\",rot = 0, title='Gender wise customer count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.col1,df.col2).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('25 percentile : {0}'.format(movielens.rating.quantile(.25))) # 25 percentile\n",
    "print('50 percentile : {0}'.format(movielens.rating.quantile(.5))) # 50 percentile\n",
    "print('75 percentile : {0}'.format(movielens.rating.quantile(.75))) # 75 percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar\n",
    "sns.barplot(x=df.ClusterID,y=df.Amount_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist\n",
    "data3.Age.plot(kind='hist',title='histogram for Age', color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"group\", y=\"weight\", data=wt, kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist\n",
    "import seaborn as sns\n",
    "sns.distplot(target,hist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter\n",
    "data3.plot.scatter(x='Age', y='total_credit', color='c', title='scatter plot : Age vs total credit');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter\n",
    "sns.jointplot(raw.PerCapGDP, raw.AvgCancerSpend )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar for male_female\n",
    "pd.crosstab(data3.Gender, data3.Behavior).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(raw, col=\"ACTIVITY\") \n",
    "g.map(plt.scatter, \"TIME\", \"SL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facet graphs\n",
    "facet = sns.FacetGrid(data3, hue=\"Behavior\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Age',shade= True)\n",
    "facet.set(xlim=(0, data3['Age'].max()))\n",
    "facet.add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair plot\n",
    "sns.set()\n",
    "sns.pairplot(dataConcrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heat map\n",
    "sns.heatmap(telecom.corr(),annot = True)\n",
    "#or\n",
    "#correration matrix\n",
    "# Ploting correlation plot\n",
    "import matplotlib.pyplot as plt\n",
    "corr=train.corr()\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "sns.heatmap(corr[(corr >= 0.5) | (corr <= -0.5)], \n",
    "            cmap='YlGnBu', vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot=True, annot_kws={\"size\": 8}, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a scatter matrix for each pair of features in the data\n",
    "pd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist\n",
    "fig, axes = plt.subplots(2, 3)\n",
    "axes = axes.flatten()\n",
    "fig.set_size_inches(18, 6)\n",
    "fig.suptitle('Distribution of Features')\n",
    "for i, col in enumerate(raw.columns):\n",
    "    feature = raw[col]\n",
    "    sns.distplot(feature, label=col, ax=axes[i]).set(xlim=(-1000, 20000),)\n",
    "    axes[i].axvline(feature.mean(),linewidth=1)\n",
    "    axes[i].axvline(feature.median(),linewidth=1, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##applying zscore\n",
    "z_data= df1.apply(zscore)\n",
    "z_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df=(df-df.mean())/df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit_transform(RFM_norm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "X = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(train_x)\n",
    "X_train = scaler.transform(train_x)\n",
    "X_test = scaler.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "variables = model2.model.exog\n",
    "\n",
    "vif2 = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n",
    "vif2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lest see multicollinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# For each X, calculate VIF and save in dataframe\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(df1.values, i) for i in range(df1.shape[1])]\n",
    "vif[\"features\"] = df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=80, svd_solver='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(z_data) # only features\n",
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cummulative sum\n",
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pca data\n",
    "pca_data=pca.fit_transform(z_data)\n",
    "pca_data# all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)# based on cummulative sum take 85+\n",
    "principalComponents = pca.fit_transform(z_data)\n",
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert above array to dataframe\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hmm model with pca on same data\n",
    "DTClassifier= DecisionTreeRegressor (max_depth=50, min_samples_split =2)\n",
    "DTClassifier.fit(principalDf,df_y)\n",
    "predicted_labels = DTClassifier.predict(principalDf)\n",
    "predicted_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test using pca\n",
    "finalDf = pd.concat([principalDf,df[\"critical_temp\"]],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ELBOW graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the screeplot - plotting the cumulative variance against the number of components\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PCA1 ,PCA2 plot\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "plt.scatter(df_train_pca[:,0], df_train_pca[:,1], c = y_train.map({0:'green',1:'red'}))\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the regression model\n",
    "learner_pca3 = LogisticRegression()\n",
    "model_pca3 = learner_pca3.fit(df_train_pca3,y_train)\n",
    "#Making prediction on the test data\n",
    "pred_probs_test3 = model_pca3.predict_proba(df_test_pca3)[:,1]\n",
    "\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before UpSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before UpSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "sm = SMOTE(sampling_strategy = 1 ,k_neighbors = 5, random_state=1)   #Synthetic Minority Over Sampling Technique\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "\n",
    "print(\"After UpSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After UpSampling, counts of label '0': {} \\n\".format(sum(y_train_res==0)))\n",
    "\n",
    "\n",
    "\n",
    "print('After UpSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After UpSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "knn = KNeighborsClassifier()\n",
    "scores = cross_val_score(knn, features, label, cv=10, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT\n",
    "DTClassifier_1= DecisionTreeClassifier(max_depth=12, min_samples_split = 3, min_samples_leaf=5)\n",
    "DTClassifier_1.fit (X,Y)\n",
    "predicted_labels = DTClassifier_1.predict(X)\n",
    "accuracy_score_1 = accuracy_score(Y, predicted_labels)\n",
    "print(accuracy_score_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "NNH = KNeighborsClassifier(n_neighbors= 10 , weights = 'uniform', metric='euclidean',p=3)\n",
    "NNH.fit(X,Y)\n",
    "predicted_labels = NNH.predict(X)\n",
    "accuracy_score_knn= accuracy_score(Y, predicted_labels)\n",
    "print(accuracy_score_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DT for train test\n",
    "DTClassifier_5= DecisionTreeClassifier (max_depth=5, min_samples_split =3,criterion='gini',min_samples_leaf=9)\n",
    "DTClassifier_5.fit(X_train,y_train)\n",
    "predicted_labels = DTClassifier_5.predict(X_test)\n",
    "accuracy_score_4 = accuracy_score(y_test, predicted_labels)\n",
    "print(accuracy_score_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic \n",
    "# Fit the model on 30%\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "model_score = model.score(X_test, y_test)\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(logreg, 13)             # running RFE with 13 variables as output\n",
    "rfe = rfe.fit(X,y)\n",
    "print(rfe.support_)           # Printing the boolean results\n",
    "print(rfe.ranking_)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification one\n",
    "#Comparing the model with StatsModels\n",
    "#logm4 = sm.GLM(y_train,(sm.add_constant(X_train[col])), family = sm.families.Binomial())\n",
    "import statsmodels.api as sm\n",
    "logm4 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\n",
    "modres = logm4.fit()\n",
    "logm4.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "regression3=ElasticNet()\n",
    "regression3.fit(X_train,y_train)\n",
    "y_pred_elasticnet=regression3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, make_scorer\n",
    "from tqdm import tqdm\n",
    "SEED = 1\n",
    "model_full_rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=SEED, n_jobs=-1)\n",
    "model_full_rf.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full_rf.score(X_test_res, y_test_res)\n",
    "model_full_rf.score(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbcl = GradientBoostingClassifier(n_estimators = 50, random_state=1)\n",
    "gbcl = gbcl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels5 = gbcl.predict(X_test)\n",
    "metrics.accuracy_score(predicted_labels5,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bgcl = BaggingClassifier(n_estimators=100, max_samples=.50 , oob_score=True , random_state=100)\n",
    "bgcl = bgcl.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred= bgcl.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score = accuracy_score(y_test,test_pred)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTClassifier= DecisionTreeClassifier (max_depth=9, min_samples_split =3,criterion='gini',min_samples_leaf=2)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abcl = AdaBoostClassifier(base_estimator=DTClassifier, n_estimators=50, random_state=1)\n",
    "#abcl = AdaBoostClassifier(n_estimators=50)\n",
    "abcl = abcl.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(criterion = 'entropy',random_state=1, max_depth = 5)\n",
    "lrcl = LogisticRegression(random_state=1)\n",
    "rfcl = RandomForestClassifier(random_state=1)\n",
    "nbcl = GaussianNB()\n",
    "bgcl = BaggingClassifier(base_estimator=dt_model, n_estimators=50 , random_state=1)  \n",
    "#the base_estimator can be null. The bagging classifer  will build it's own tree\n",
    "\n",
    "enclf = VotingClassifier(estimators = [('lor', lrcl), ('rf', rfcl), ('nb', nbcl), ('bg', bgcl)], voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf, label in zip([lrcl , rfcl, nbcl, enclf, bgcl], ['Logistic Regression', 'RandomForest', 'NaiveBayes', 'Ensemble', 'Bagging']):\n",
    "    scores = cross_val_score(clf, X, Y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.02f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicted probabulity on clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities\n",
    "y_pred = logsk.predict_proba(X_test)\n",
    "# Converting y_pred to a dataframe which is an array\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "# Converting to column dataframe\n",
    "y_pred_1 = y_pred_df.iloc[:,[1]]\n",
    "# Let's see the head\n",
    "y_pred_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting CustID to index\n",
    "y_test_df['CustID'] = y_test_df.index\n",
    "# Removing index for both dataframes to append them side by side \n",
    "y_pred_1.reset_index(drop=True, inplace=True)\n",
    "y_test_df.reset_index(drop=True, inplace=True)\n",
    "# Appending y_test_df and y_pred_1\n",
    "y_pred_final = pd.concat([y_test_df,y_pred_1],axis=1)\n",
    "# Renaming the column \n",
    "y_pred_final= y_pred_final.rename(columns={ 1 : 'Churn_Prob'})\n",
    "# Rearranging the columns\n",
    "y_pred_final = y_pred_final.reindex_axis(['CustID','Churn','Churn_Prob'], axis=1)\n",
    "# Let's see the head of y_pred_final\n",
    "y_pred_final.head()\n",
    "## row id,actual value,predicted prob by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new column 'predicted' with 1 if Churn_Prob>0.5 else 0\n",
    "y_pred_final['predicted'] = y_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.5 else 0)\n",
    "# Let's see the head\n",
    "y_pred_final.head()\n",
    "##row id,actual value,predicted prob,predicted_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classfication report\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, make_scorer\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_pred_final.Churn, y_pred_final.predicted)\n",
    "print(metrics.confusion_matrix(y_test, y_predict))\n",
    "model_score = model.score(X_test, y_test)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making prediction on the test data\n",
    "pred_probs_test = model_pca.predict_proba(df_test_pca)[:,1]\n",
    "\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "np.mean(np.square(data.y - testpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "rmse( y, lmpredict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(df_y,predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSE\n",
    "SSE=sum(np.square(raw1.Temp - predvalues1))\n",
    "SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "np.mean(np.square(raw1.Temp - predvalues1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_confusion_matrix(y_test_res, y_pred, normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_pred_final.Churn, y_pred_final.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1. Accuracy: \", str(metrics.accuracy_score(y_test,y_pred) * 100) + \"%\")\n",
    "print()\n",
    "\n",
    "precision = metrics.precision_score(y_test, y_pred, average=None) * 100\n",
    "print(\"2. Precision: \")\n",
    "for i in range(3):\n",
    "    print(data.target_names[i] + \": \" + str(precision[i]) + \"%\")\n",
    "print()\n",
    "    \n",
    "recall = metrics.recall_score(y_test, y_pred, average=None) * 100\n",
    "print(\"3. Recall: \")\n",
    "for i in range(3):\n",
    "    print(data.target_names[i] + \": \" + str(recall[i]) + \"%\")\n",
    "print()\n",
    "\n",
    "print(\"4. Confusion Matrix: \")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_roc(y_pred_final.Churn, y_pred_final.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,len(submission)):\n",
    "#    submission[\"Ref.No\"][i] = i+1\n",
    "#submission[\"Y\"]=predicted_y_labels2\n",
    "#submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission.to_csv('myown.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xelatex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=546789)\n",
    "sub_preds_rf = np.zeros((test.shape[0], 9))\n",
    "oof_preds_rf = np.zeros((train.shape[0]))\n",
    "score = 0\n",
    "for i, (train_index, test_index) in enumerate(folds.split(train, target['target_feature'])):\n",
    "    print('-'*20, i, '-'*20)\n",
    "    \n",
    "    clf =  RandomForestClassifier(n_estimators = 350, n_jobs = -1)\n",
    "     #clf = GradientBoostingClassifier(n_estimators = 150, random_state=1)\n",
    "    clf.fit(tr.iloc[train_index], target['surface'][train_index])\n",
    "    oof_preds_rf[test_index] = clf.predict(tr.iloc[test_index])\n",
    "    sub_preds_rf += clf.predict_proba(te) / folds.n_splits\n",
    "    score += clf.score(tr.iloc[test_index], target['surface'][test_index])\n",
    "    print('score ', clf.score(tr.iloc[test_index], target['surface'][test_index]))\n",
    "    importances = clf.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    features = tr.columns\n",
    "\n",
    "    hm = 30\n",
    "    plt.figure(figsize=(7, 10))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices[:hm])), importances[indices][:hm], color='b', align='center')\n",
    "    plt.yticks(range(len(indices[:hm])), [features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "\n",
    "print('Avg Accuracy', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss['surface'] = le.inverse_transform(sub_preds_rf.argmax(axis=1))\n",
    "ss.to_csv('rf92_feature_submission.csv', index=False)\n",
    "ss.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.33,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.05,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': -1,\n",
    "    'metric':'auc',\n",
    "    'min_data_in_leaf': 80,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 13,\n",
    "    'num_threads': 12,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary',\n",
    "    'verbosity': 1\n",
    "}\n",
    "num_round = 100000\n",
    "# check random state 44000\n",
    "folds = StratifiedKFold(n_splits=12, shuffle=False, random_state=12345)\n",
    "oof = np.zeros(len(train_df))\n",
    "predictions = np.zeros(len(test_df))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 2500)\n",
    "    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9multiclass\n",
    "params = {\n",
    "    'num_leaves': 10,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'objective': 'multiclass',\n",
    "    'max_depth': 17,\n",
    "    'learning_rate': 0.01,\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"bagging_freq\": 7,\n",
    "    \"bagging_fraction\": 0.8126672064208567,\n",
    "    \"bagging_seed\": 11,\n",
    "    \"verbosity\": -1,\n",
    "    'reg_alpha': 0.1302650970728192,\n",
    "    'reg_lambda': 0.3603427518866501,\n",
    "    \"num_class\": 9,\n",
    "    'nthread': -1\n",
    "}\n",
    "\n",
    "def multiclass_accuracy(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    pred_class = np.argmax(preds.reshape(9, -1).T, axis=1)\n",
    "    return 'multi_accuracy', np.mean(labels == pred_class), True\n",
    "\n",
    "t0 = time.time()\n",
    "train_set = lgb.Dataset(train_df, label=target)\n",
    "eval_hist = lgb.cv(params, train_set, nfold=10, num_boost_round=9999,\n",
    "                   early_stopping_rounds=100, seed=19, feval=multiclass_accuracy)\n",
    "num_rounds = len(eval_hist['multi_logloss-mean'])\n",
    "# retrain the model and make predictions for test set\n",
    "clf = lgb.train(params, train_set, num_boost_round=num_rounds)\n",
    "predictions = clf.predict(test_df, num_iteration=None)\n",
    "print(\"Timer: {:.1f}s\".format(time.time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1, v2 = eval_hist['multi_logloss-mean'][-1], eval_hist['multi_accuracy-mean'][-1]\n",
    "print(\"Validation logloss: {:.4f}, accuracy: {:.4f}\".format(v1, v2))\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title(\"CV multiclass logloss\")\n",
    "num_rounds = len(eval_hist['multi_logloss-mean'])\n",
    "ax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_logloss-mean'])\n",
    "ax2 = ax.twinx()\n",
    "p = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_logloss-stdv'], ax=ax2, color='r')\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title(\"CV multiclass accuracy\")\n",
    "num_rounds = len(eval_hist['multi_accuracy-mean'])\n",
    "ax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_accuracy-mean'])\n",
    "ax2 = ax.twinx()\n",
    "p = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_accuracy-stdv'], ax=ax2, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'gain': clf.feature_importance(importance_type='gain'),\n",
    "                           'feature': clf.feature_name()})\n",
    "importance.sort_values(by='gain', ascending=False, inplace=True)\n",
    "plt.figure(figsize=(10, 20))\n",
    "ax = sns.barplot(x='gain', y='feature', data=importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
